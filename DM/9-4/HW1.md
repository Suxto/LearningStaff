# 9-4作业

### 1. Web发展历史

万维网的创立:万维网最初是由 Tim Berners-Lee 于1989 年发明的。当时，他在位于瑞士的欧洲粒子物理实验室工作。他给万维网命名，并且编写了世界上首个万维网服务器httpd 和世界上首个客户序 “World  Wide Web” (包括一个浏览器和一个编辑器)。
事件起源于1989年3月，当时 Tim Berners-Lee 向他在CERN的导师提交了一份名为”信息管理提议“的提议书。在这份提议中，他讨论了层次化信息组织的缺点，并且描绘出基于超文本系统的优点。提议书建议设计一套简单的协议，使得用户可以通过网络请求存放在远端系统上的信息；并创立一套使信息可以用**相同格式被互相交换**，并且用户可以通过超链接把相关文档链接起来的机制。其中还提到如何使用当时在CERN 的一些文本阅读和图形显示的技术。提议书完整地描述了分布式超文本系统(Distributed Hypertext System)，也就是当今万维网的基础构架。

### 2. 分析WEB1.0、WEB2.0、WEB3.0的区别

**WEB1.0：**

- 时间范围：大约从互联网出现到2000年左右。
- 特点：WEB1.0时代是互联网的初始阶段，主要以静态网页为主，用户只能 passively（被动地）浏览网页内容，无法进行实时互动或用户生成内容。
- 内容创作：网站内容主要由专业开发人员和机构创建和发布。
- 用户角色：用户主要是信息的接收者，缺乏参与性和互动性。

**WEB2.0：**

- 时间范围：大约从2000年左右持续到2010年左右，直至今天。
- 特点：WEB2.0引入了互联网的新一代技术和思维方式，强调用户**生成内容、社交互动和在线合作**。
- 内容创作：用户可以通过各种方式创建和分享内容，如社交媒体、博客、维基百科等。
- 用户角色：用户不仅仅是信息的接收者，还是内容的**创造者和共享者**，可以积极参与互联网社区，进行评论、分享和互动。

**WEB3.0：**

- 时间范围：WEB3.0是一个正在发展的概念，目前尚未明确定义时间范围。
- 特点：WEB3.0被视为下一代互联网技术和应用的演进，强调个人数据的掌控、去中心化、智能合约和区块链技术的应用。
- 内容创作：用户可以通过去中心化的应用程序创建和管理内容，同时保护个人数据的隐私和安全。
- 用户角色：用户在WEB3.0中更加拥有数据的主权和控制权，可以更好地参与经济交易、数据治理和社交互动。

### 3. 描述数据挖掘及其主要步骤

+ **描述**

  数据挖掘又称为数据库知识发现 (Knowledge Discovery in Databases，KDD)。它通常是指从数据源(如数据库、文本、图片、万维网等)中探寻有用的模式(Patterns)或知识的过程。这些模式必须是有用的、有潜在价值的，并且是可以被理解的。数据挖掘是一门多学科交叉的学问，包括：机器学习、统计、数据库、人工智能、信息检索和可视化。

+ **步骤**

  1. **预处理:**原始数据通常都不适合直接用来挖掘。其中有多个原因。首先，它可能包含噪音和异常情况；必须经过过滤。其次，数据量可能非常大，并且包含有不相关的属性;需要通过采样和选择特定属性来降低数据量。
  2. **数据挖掘:**然后将经过预处理的数据送到数据控掘算法中，生成模式或知识
  3. **后续处理:**在许多应用中，并不是所有被发现的模式都是有用的。这个步骤就是要识别出其中的有用部分。有很多评估和可视化的技术可以被用来做此项决策。

### 4. 简述下面数据挖掘技术的主要任务：

1. **分类（Classification）：**
   + 主要任务：分类是一种监督学习任务，旨在根据已知的样本和其相应的标签，构建一个模型来预测新样本的类别或类标签。
   + 应用领域示例：垃圾邮件过滤、文本分类、疾病诊断、信用评分等。

2. **回归（Regression）：**
   + 主要任务：回归也是一种监督学习任务，旨在通过已知的输入特征和其相应的**连续输出**，建立一个模型来**预测新样本**的数值输出。
   + 应用领域示例：房价预测、销售量预测、股票价格预测、天气预测等。

3. **聚类（Clustering）：**
   + 主要任务：聚类是一种无监督学习任务，旨在将相似的样本分组成为簇，使得同一簇内的样本相互之间更加相似，而不同簇之间的样本差异较大。
   + 应用领域示例：市场细分、社交网络分析、图像分割、用户行为分析等。

4. **关联规则挖掘（Association Rule Mining）：**
   + 主要任务：关联规则挖掘旨在发现数据集中的频繁项集和关联规则，以揭示数据集中的关联关系和相关性。
   + 应用领域示例：购物篮分析、交叉销售推荐、网络用户行为分析、医学诊断支持等。

### 5. 传统数据挖掘中数据预处理主要任务

1. **数据清洗（Data Cleaning）：**
   - 任务：处理数据中的噪声、缺失值、异常值等问题，以确保数据的质量和准确性。
   - 操作：删除重复记录、纠正错误数据、填补缺失值、处理异常值等。
2. **数据集成（Data Integration）：**
   - 任务：将来自不同数据源的数据合并成一个一致的数据集，以便进行后续的分析和挖掘。
   - 操作：解决数据命名不一致、格式不同、数据结构不匹配等问题，进行数据表连接、合并等操作。
3. **数据转换（Data Transformation）：**
   - 任务：将数据转换为适合数据挖掘算法使用的形式，以提高模型的性能和效果。
   - 操作：进行数据规范化、属性变换、离散化、连续属性到符号属性的映射等操作。
4. **数据规约（Data Reduction）：**
   - 任务：通过减少数据的规模和复杂度，以降低计算成本和提高挖掘效率。
   - 操作：采样、维度约简、特征选择等方法来减少数据量和特征数量。

### 6.WEB数据挖掘可以分为几种主要类型？

+ **Web 结构挖掘：** Web结构挖掘从表征Web结的超**链接**(简称链接)中寻找有用的知识。例如: 从这些链接中可以找出哪些是重要的网页一-这是一项搜索引擎采用的重要技术，也可以发掘具有共同兴趣的用户社区。这些任务在传统的数据挖掘中并不存在，因为在关系型表格中并没有链接结构。
+ **Web内容挖掘：** Web内容挖掘从网页内容中抽取有用的**信息和知识**。例如:根据网页的主题，可以自动进行聚类和分类。虽然这些任务与传统数据挖掘的任务相似，但是我们依然可以为了各种不同的目的从网页中根据模式抽取有用的信息，例如商品描述、论坛回帖等，而这些信息可以被用做进一步分析来挖掘用户态度。这些任务也不是传统的数据挖掘任务。
+ **Web 使用挖掘：** Web使用挖掘从记录每位用户点击情况的使用日志中挖掘用户的**访问模式**。这项任务也使用了许多数据挖掘的算法。其中一项重要的议题是点击流数据的预处理，以便生成可以用来挖掘的合适数据

### 7. 本课程主要内容可以分为几部分？

两大主要部分组成。

+ **第1部分**包括第 2~5 章，介绍数据挖掘的主要内容。
+ **第2部分**，也就是剩下的章节，介绍 Web 挖掘。

### 8. 什么是监督学习和无监督学习

+ **监督学习(Supervised Learning):**监督学习或许是数据挖掘和 Web 挖掘中最常用的挖掘/学习技术了。它也称为分类，旨在从预先标有类别信息的数据中学得一种**分类函数**(也称分类器 (Classifier))。然后将该分类器应用在未知类别信息的数据上，从而将它们分到各个类别中。由于学习过程使用的数据(称为训练数据)事先**经过标注**，因此这种方法被称为监督学习。
+ **无监督学习(Unsupervised Learning):**在无监督学习中，使用的数据并**没有经过事先标注**。学习算法必须要找出隐藏在数据中的结构或规律。无监督学习中的一项重要技术就是**聚类**(Clustering)，它将数据对象根据相似程度(或相异程度)来组织成小组或类。聚类在 Web 挖掘中被广泛使用。例如，当我们将网页聚类成一个个小组以后，在同一小组中的网页就代表一个特定的主题。也可以将网页聚成带有层次的类，这样就代表了个层次化的主题结构。

